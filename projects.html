<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta charset="utf-8">
  <title>Victor Sindato</title>
  <!-- CSS only -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
    integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
  <link rel="stylesheet" href="index.css">
</head>

<body class="ml-4 pl-4 mt-4 pt-4">
  <div class="row">
    <!-- Menu -->
    <div class="col-2">
      <h6 class="m-0 p-0">victor sindato</h6>
      <div>
        <a href="./index.html">welcome</a>
      </div>
      <div>
        <a href="#">projects</a>
      </div>
      <div>
        <a href="./writings.html">writings</a>
      </div>
      <div>
        <a href="./reads.html">reads</a>
      </div>
      <div>
        <a href="./stash.html">stash</a>
      </div>
    </div>

    <!-- Content -->
    <div class="col-6">
      <ul class="list-group list-group-flush">
        <!-- Chipper Cash -->
        <li class="list-group-item pt-0">
          <h4>Chipper Cash</h4>
          <p><b>Location</b>: San Francisco, CA (Remote) | July 2021 - Current </p>
          <p><b>Context</b>: <a href="https://chippercash.com/" alt="Chipper Cash" target="_blank">website</a></p>
          <p><b>Description</b><br>
            Building pieces of the data infrastructure that supports our growth and compliance efforts
          </p>
          <p><b>Tools</b>: Python, Node.js, SQL </p>
        </li>
        <!-- Sauce-pricing -->
        <li class="list-group-item">
          <h4>Sauce</h4>
          <p><b>Location</b>: Los Angeles, CA (Remote) | January 2021 </p>
          <p><b>Context</b><br>
            Dynamic pricing is an umbrella term for the pricing strategies that adapt prices of products to their
            current market demand.
            Amazon, Uber, and Airbnb use this for online shopping, ride-hailing, and vacation rentals respectively.
            <a href="https://www.saucepricing.com/" alt="Sauce pricing" target="_blank">Sauce</a> aims at applying those
            techniques to restaurants' online.
          </p>
          <p><b>Description</b><br>
            In this month-long project, I worked closely with the co-founders in implementing the customer-facing
            front-end of the product.
            We used React.js for building the product's pages. We then used Apollo and GraphQL for querying and mutating
            data from and to the database
            which was hosted on MongoDB. This was an immense learning experience and have grown confident with back-end
            dev through it.
          </p>
          <p><b>Tools</b>: Javascript, React.js, Apollo, GraphQL, MongoDB</p>
        </li>
        <!-- Exploring fairness in Aurora -->
        <li class="list-group-item">
          <h4>Fair Aurora</h4>
          <p><b>Location</b>: Networks course | Cambridge, MA | Fall 2020 </p>
          <p><b>Context</b><br> At its core, the problem of congestion control is about managing the behavior of senders
            in a network to optimize for throughput and fairness. Tens of protocols have been designed since the birth
            of TCP, some being variants of it and others novel. <a
              href="https://pbg.cs.illinois.edu/papers/jay19aurora.pdf" target="_blank">Aurora </a> is one such
            protocol. Introduced in 2019, it was inspired by the recent success of reinforcement learning in domains
            that involve long-term decision-making. Aurora’s major strength lies in its ability to quickly adapt to new
            network conditions such as changes in link bandwidths, queue sizes, latencies and packet losses. When
            evaluated alongside standard protocols such as TCP-CUBIC, that adaptability has been shown to give it a
            significant edge in terms of link utilization.</p>
          <p><b>Description</b><br>
            An open challenge that remains is designing a solution to ensure that Aurora senders share the network's
            resources fairly with senders using other protocols. The reward function that’s used in training Aurora is
            solely focused on ensuring high throughput, low latency, and fewer packet losses. If placed alongside
            senders using TCP, it’s been rightfully speculated that given what that reward function emphasizes, Aurora
            would learn to behave in a way that causes those TCP senders to always back-off in order to get higher
            values of the reward function. We consider this an opportunity to explore alternative reward functions that
            could be used to train Aurora such that it not only prioritizes maximizing throughput but also learns how
            many other senders it’s sharing, say, a bottleneck link with and how it can balance maximizing throughput
            and fairly sharing that available bandwidth.
          </p>
          <p><b>Tools</b>: Git, Python</p>
          <p><b>Results</b>: <a href="./papers/fair-aurora-report.pdf" target="blank">Report</a></p>
          <p><b>Resources</b>: <a href="https://pbg.cs.illinois.edu/papers/jay19aurora.pdf" target="blank">Aurora
              paper</a>
        </li>
        <!-- ABR -->
        <li class="list-group-item">
          <h4>Bitrate selection</h4>
          <p><b>Location</b>: Networks class | Cambridge, MA | Fall 2020 </p>
          <p><b>Context</b><br> 6.829 is a project-based networks course that covers a broad range of core topics
            including congestion control (both end-to-end and network-assisted), routing, decentralized systems, and
            more. </p>
          <p><b>Description</b><br>
            The video streaming set-up is as follows: There's a client (eg. youtube app) that maintains a buffer of some
            size. As the user is watching a video, the client is actively downloading packets from a server (eg. youtube
            server) over a link/connection that has a varying throughput. These incoming packets, encoded in multiple
            bitrates, are queued in the client buffer - waiting to be played. The main question is at what bitrate
            should the client play the in-buffer packets to optimize an overall measure of <i>Quality of Experience</i>
            (QoE). More of the problem description can be found <a
              href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming" target="blank">here</a>.
          </p>
          <p><b>Tools</b>: Git, Python</p>
          <p><b>Results</b>: The course staff prepared the code that simulated the client and network. I implemented a
            feedback-control based approach which maintained a running average of the connection capacity and nudged the
            selected bitrate towards that average.</p>
          <p><b>Resources</b>: <a href="https://github.mit.edu/vsindato/6.829-lab3" target="blank">Project repo</a>
        </li>
        <!-- School-book -->
        <li class="list-group-item">
          <h4>School-book</h4>
          <p><b>Location</b>: MIT Media Lab | Cambridge, MA | Summer 2020 </p>
          <p><b>Context</b><br> The Media Lab is an anti-disciplinary research lab with dozens of groups working at the
            intersection of technology, media, science, art, and design. The Personal Robots Group works on developing
            fun and innovative ways of teaching AI across all levels of education. </p>
          <p><b>Description</b><br>
            A study on misinformation, conducted by a different group, showed that misinformation spreads about 6 times
            faster than real information. This can have serious implications when it comes to activities, such as
            voting, protesting, etc, that involve collective decision-making. The goal of this NSF-funded project was to
            raise awareness in young children about the threats of misinformation and how they can be more alert when
            they're online. We ended up developing School-book, a fun headline-sharing platform, which allowed students
            to directly witness how the information they shared with others spread in their network. I was responsible
            for designing and developing School-book’s front-end and back-end.
          </p>
          <p><b>Tools</b>: Git, Python, Javascript, Socket.io, React.js, Express.js</p>
          <p><b>Results</b>: In a period of ~2 months, we used school-book in 5 workshops with over 100 students total.
          </p>
          <p><b>Resources</b>: <a href="./papers/school-book.pdf" target="blank">Demo paper</a> | <a
              href="https://github.com/vsindato/misinformation" target="blank">Code</a> | <a
              href="https://aieducation.mit.edu/daily/index.html" target="blank">Project page</a></p>
        </li>
        <!-- Affdex+TeleHealth -->
        <li class="list-group-item">
          <h4>Affdex</h4>
          <p><b>Location</b>: Affectiva | Remote | Summer 2020 </p>
          <p><b>Context</b><br> Affectiva is an AI company that uses and develops computer vision and natural language
            processing technologies, combined with a lot of in-house curated datasets to do affect appraisal along
            multiple modalities. Their software’s applications span multiple industries including gaming, robotics,
            education, healthcare, experiential marketing, retail, human resources etc. </p>
          <p><b>Description</b><br>
            On my team’s month-long project, we set out to apply Affdex, Affectiva’s tool that’s tuned to recognize a
            host of facial expressions, to virtual patient-doctor interactions. Our solution was a web application that
            executed real-time emotion recognition over 1:1 video calls. From the doctor’s side, we used Affectiva’s
            Affdex SDK and Zoom’s web SDK to process the video feeds from them and their patient. We then slightly
            modified the Zoom interface by adding a ‘Get Analysis’ button that the doctor could use at the end of the
            session to see charts of the session’s expressions by both themselves and their patient.
          </p>
          <p><b>Tools</b>: Git, HTML+CSS, Javascript, SDKs</p>
          <p><b>Results</b>: My team and I presented to a panel of Affectiva engineers, some of whom developed Affdex.
          </p>
          <p><b>Resources</b>: <a href="https://github.com/vsindato/Goleador" target="blank">Code</a></p>
        </li>

        <!-- Maze-escape -->
        <li class="list-group-item">
          <h4>Maze-escape</h4>
          <p><b>Location</b>: MIT | Cambridge, MA | Fall & Spring 2019 </p>
          <p><b>Context</b><br> I did this project as a follow-up to the cartpole-balancing project I write about below
          </p>
          <p><b>Description</b><br>
            My objective was to get hands-on experience with some elementary model-free RL algorithms such as Q-learning
            by applying them to the task of learning the shortest-path out of a maze.
          </p>
          <p><b>Tools</b>: Git, Python, OpenAI Gym</p>
          <p><b>Results</b>: I was fortunate to deliver a brief presentation on it in a project-based class </p>
          <p><b>Resources</b>: <a href="https://github.com/vsindato/maze-escape" target="blank">Code</a> | <a href="#"
              target="blank">Demo</a> </p>
        </li>

        <!-- Cartpole-balancing -->
        <li class="list-group-item">
          <h4>Cartpole-balancing</h4>
          <p><b>Location</b>: MIT | Cambridge, MA | Fall 2019 </p>
          <p><b>Context</b><br> After taking 6.036 (MIT's introductory ML class), I developed an interest in
            reinforcement learning. RL is about training an agent to make a series of optimal decisions so that it's
            able to achieve some long-term objective. Examples of areas that RL has been applied include games,
            robotics, trading, advertising and many others.</p>
          <p><b>Description</b><br>
            For this project, my objective was to apply this learning technique to a simpler environment, namely,
            balancing a pole on a moving cart. A full description of the cartpole problem can be found <a
              href="https://github.com/openai/gym/wiki/CartPole-v0">here</a>
          </p>
          <p><b>Tools</b>: Git, Python, OpenAI Gym</p>
          <p><b>Results</b>: This was my first RL project! It was more of a hands-on learning experience than anything
            else. </p>
          <p><b>Resources</b>: <a href="https://github.com/vsindato/cartpole-balancing" target="blank">Code</a></p>
        </li>
      </ul>
    </div>

    <div class="col-4">
      <a class="pt-4 sticky-top" href="https://github.com/vsindato">[Github]</a>
    </div>

</body>

</html>